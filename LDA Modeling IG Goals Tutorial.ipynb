{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA topic modeling goals\n",
    "\n",
    "\n",
    "The utility of topic modeling methods is their capability to uncover unobserved variables—topics—which shape the meaning of textual documents. Modern-day scholars utilize topic modeling to uncover latent topics from a wide array of textual information—from shorter texts, such as twitter posts to longer texts, such as journal articles.\n",
    "\n",
    "\n",
    "\n",
    "This notebook applies LDA modeling to an experimental dataset investigating participants' goal inferences. \n",
    "\n",
    "\n",
    "### Key python libraries:\n",
    "- gensim (https://radimrehurek.com/gensim/)\n",
    "- nltk (https://www.nltk.org)\n",
    "- spacy (https://spacy.io)\n",
    "\n",
    "### Helpful Links:\n",
    "- https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n",
    "- https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## A Comprehensive Example:\n",
    "\n",
    "The data represent the responses of 119 participants to the questionnaire described in the paper \"A Theory-Driven Computational Measure of Goal Activation in Communication Science\". \n",
    "\n",
    "\n",
    "Participants were asked to list all the goals they could think of for a total of four time points. Each goal serves as a single document, leaving us with a total of 2976 documents—each participant providing up to 40 documents, across four time points. \n",
    "\n",
    "\n",
    "LDA assumes that a single document can contribute to multiple topics simultaneously; in other words, LDA explicitly models the actual distribution of words within each document. The aim of the analysis is to investigate participants’ open-ended responses to the questionnaire.\n",
    "\n",
    "###  Steps of the analysis:\n",
    "\n",
    "#### 1. Preparing data for LDA\n",
    "    a. Spell check\n",
    "    b. Expand contractions\n",
    "    c. Read the data \n",
    "    d. Check data integrity\n",
    "    e. Delete missing values\n",
    "#### 2. Text preprocessing\n",
    "    a. Tokenization\n",
    "    b. Lemmatization  \n",
    "    c. Stop Word Removal\n",
    "    d. Bigrams and Trigrams\n",
    "    e. Exclude terms in > 99% and < 1% of documents\n",
    "    f. Generate Corpus and Dictionary\n",
    "#### 3. Model selection (Selecting the number of topics (k))\n",
    "    a. Computing Model Perplexity\n",
    "    b. Analyzing model results through pyLDAvis visualization\n",
    "    c. Saving selected model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Required Libraries\n",
    "\n",
    "#general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "\n",
    "#setting up Jupyter notebook \n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 5000)\n",
    "pd.set_option('display.width', 10000)\n",
    "\n",
    "#text preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "\n",
    "#modeling\n",
    "import gensim\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "\n",
    "#plotting\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.0/en_core_web_md-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing data for LDA\n",
    "\n",
    "Before you read in your data, you should manually run your textual data through a spell checker in order to take advantage of the semantic and syntactic context when selecting the proper correct spelling.\n",
    "\n",
    "Similar to the spellchecker, we needed human coders to expand all English contractions (e.g., \"don't\" -> \"do not\"), to ensure accuracy.\n",
    "\n",
    "After you read the data in, you should check its integrity to avoid unexpected and artificial errors and delete missing values (null values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File paths\n",
    "\n",
    "#data files\n",
    "file_location = './data/experimental_data.xlsx'\n",
    "\n",
    "#stop words\n",
    "stopwords_location = './data/stopwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check to make sure the dataset looks correct\n",
    "try:\n",
    "    data = pd.read_excel(file_location, encoding='latin1')\n",
    "    print(\"{} Rows.  {} Columns.\".format(*data.shape))\n",
    "except:\n",
    "    print(\"Dataset could not be loaded. Is the dataset missing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spot checks\n",
    "indices = [0,333,777]\n",
    "\n",
    "samples = pd.DataFrame(data.loc[indices, :], columns = data.keys()).reset_index(drop = True)\n",
    "print(\"Sample Tickets:\")\n",
    "display(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Check number of null values in each column of the full dataset\n",
    "pd.DataFrame(data.isnull().sum(), columns=['Number of NULL values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['goals'] = data['goals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove missing (null) values from the data\n",
    "\n",
    "#finding null values in the full dataset\n",
    "print(\"=============Full Dataset=============\")\n",
    "data['goals'] = data['goals']\n",
    "\n",
    "print('Number of rows in goals:', len(data['goals']))\n",
    "print(\"-------------------\")\n",
    "print(\"Null Values in goals: {}\".format(data['goals'].isnull().sum()))\n",
    "\n",
    "#removing null values from the full dataset\n",
    "goals = data['goals']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text preprocessing\n",
    "This step is needed to generate a ‘bag-of-words’ LDA model and it includes: text tokenization and lemmatization, removal of Stop Words and words that appear in > 99% and < 1% of documents, including bigrams and trigrams, and generating Corpus and Dictionary.\n",
    "\n",
    "**Tokenization** involves converting the text to lowercase, removing special characters, and punctuation from the text. Also, we should be careful to remove alphanumerics, numbers, words that appear in the corpus less than twice and extra spaces.\n",
    "\n",
    "\n",
    "**Lemmatization** is used reduces the size of the vocabulary in the model. It transforms words to their lemma (e.g., assaulted -> assault). So that the model can analyze several inflected forms of a word as a single word. Also, lemmatization using Spacy allows to select certain part of speech words (e.g., noun, adj, vb, adv).\n",
    "\n",
    "\n",
    "**Stop Word Removal** often is an important step to have a better input for modeling. Stop words are very common words in a language (e.g. a, an, the etc.). Note: you can edit the stop words txt file to add additional words to filter out.  We recommend filtering out as few stop words as possible, as even commonly occurring words can offer meaningful information, especially when responses are terse. However, depending on the specific characteristics of the textual data, stop word removal may be necessary to minimize model noise.\n",
    "\n",
    "**Bigrams and Trigrams** are two and three consequent words that frequently co-occur together.\n",
    "\n",
    "**Exclude terms in > 99% and < 1% of documents** is necessary to remove words that are contentless words in the documents. This allows to reduce model noise.\n",
    "\n",
    "\n",
    "**Corpus** is our collection of documents (i.e., our textual questionnaire responses) and <br>\n",
    "**Dictionary** is a list of unique words in the corpus. It takes each unique word in the corpus and assigns them an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the text to lowercase\n",
    "goals = goals.str.lower()\n",
    "\n",
    "print(\"=======Full Dataset==============\\n\")\n",
    "print(goals.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove all punctuation and tokenize texts\n",
    "\n",
    "#define helpful function\n",
    "def tokenize(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(simple_preprocess(str(sentence), deacc=True))  # deacc=True removes all punctuation\n",
    "\n",
    "#tolkenize full data set\n",
    "goals_tokens = list(tokenize(goals))\n",
    "\n",
    "\n",
    "print(\"\\n[INFO] goals....................\\n\")\n",
    "print(goals_tokens[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lemmatize words, keeping only noun, adj, vb, adv\n",
    "\n",
    "#define helpful function\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'SCONJ', 'PART', 'NOUN', 'INTJ', 'AUX', 'ADV', 'ADP', 'ADJ']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc])\n",
    "    return texts_out\n",
    "\n",
    "#initialise Spacy\n",
    "import spacy \n",
    "import en_core_web_md \n",
    "#Initialize spacy model, keeping only tagger component (for efficiency)\n",
    "nlp = en_core_web_md.load(disable=['parser', 'ner'])\n",
    "\n",
    "#lemmatize and select only noun, adj, vb, adv\n",
    "goals_lemma = lemmatization(goals_tokens, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'SCONJ', 'PART', 'NOUN', 'INTJ', 'AUX', 'ADV', 'ADP', 'ADJ'])\n",
    "print(str(len(goals_lemma)))\n",
    "print(goals_lemma[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare to remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "chachedWords = stopwords.words('english')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "newStopWords =[str(x.strip()) for x in open(stopwords_location,'r').read().split('\\n')]\n",
    "stopwords.update(newStopWords)\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove stop words \n",
    "\n",
    "#define helpful function\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stopwords] for doc in texts]\n",
    "\n",
    "#remove stop words \n",
    "goals_stopwords = remove_stopwords(goals_lemma)\n",
    "\n",
    "print(\"\\n[INFO] goals....................\\n\")\n",
    "print(goals_stopwords[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select Trigrams and Bigrams\n",
    "            \n",
    "goals_bigram = Phrases(goals_stopwords, min_count=3, delimiter=b' ', threshold=1)\n",
    "goals_trigram = Phrases(goals_bigram[goals_stopwords], threshold=1)\n",
    "\n",
    "goals_bigram_mod = gensim.models.phrases.Phraser(goals_bigram)\n",
    "goals_trigram_mod = gensim.models.phrases.Phraser(goals_trigram)\n",
    "\n",
    "for idx in range(len(goals_stopwords)):\n",
    "    for token in goals_trigram_mod[goals_bigram_mod[goals_stopwords[idx]]]:\n",
    "        #print(token)\n",
    "        if ' ' in token:\n",
    "            goals_stopwords[idx].append(token)\n",
    "print(\"\\n[INFO] goals....................\\n\")\n",
    "print(goals_stopwords[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Corpus\n",
    "dictionary_goals = gensim.corpora.Dictionary(goals_stopwords)\n",
    "dictionary_goals.filter_extremes(no_below=.01, no_above=0.99)\n",
    "\n",
    "## Generate Dictionary\n",
    "corpus_goals = [dictionary_goals.doc2bow(text) for text in goals_stopwords]\n",
    "\n",
    "## Save Corpus and Dictionary on a local drive\n",
    "pickle.dump(corpus_goals, open('./output/corpus_goals.pkl', 'wb'))\n",
    "dictionary_goals.save('./output/dictionary_goals.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model selection (Selecting the number of topics (k))\n",
    "A Model is represented by model Hyperparameters that define prior distribution of the topics within each document, and prior distribution of the different words within each topic. These should be defined based on theoretical assumptions about how we think the topics are actually distributed amongst our data. LDA model from gensim library has the following Hyperparameters:\n",
    "- **Beta** (referred to as 'eta' in gensim) = the [distribution of the] number of words per topic\n",
    "- **Alpha** =  the [distribution of the] number of topics per document\n",
    "\n",
    "\n",
    "\n",
    "Both alpha and eta can be set to ‘symmetric’, ‘asymmetric’, or ‘auto’, where:\n",
    "- ‘auto’ = the model learns the best values for the hyperparameters as it is trained on more and more data (i.e., it learns an asymmetric prior from the corpus). See http://jonathan-huang.org/research/dirichlet/dirichlet.pdf for an overview             \n",
    "- 'asymmetric' = uses a fixed, normalized asymmetric prior of 1.0 / k (number of topics)\n",
    "- 'symmetric' = uses a distribution of 1 / k (number of topics)\n",
    "\n",
    "\n",
    "\n",
    "In Bayesian statistics, we have to define the distributions (i.e., prior distributions) of unknown variables (e.g., ϕ and θ) before running the data analysis. These should be defined based on theoretical assumptions about how we think the topics are actually distributed amongst our data. In our case, it makes sense to assume that some documents discuss more/less topics than other documents; thus, we set the document-topic distribution to be asymmetric. \n",
    "\n",
    "\n",
    "We recommend setting alpha = 'auto' as it sets the distribution to be asymmetric, and learns the best alpha value (i.e., lowest perplexity scores) from the data itself. It also makes sense to assume that some topics contain more words than others. Thus, we recommend setting the distribution of the number of words per topic to be asymmetric as well.\n",
    "\n",
    "In addition, gensim LDA model has the following parameters:\n",
    "- **Passes** = number of laps the model goes through the entire corpus (Increasing the number of passes reduces model bias)\n",
    "- **Chunksize** = number of documents to load into memory at a time (smaller chunk sizes save memory, but take longer to train)\n",
    "- **Update_every** = number of chunks to process before maximizing your model \n",
    "- **Random state** = sets the seed to make the model reproducible\n",
    "- **Number of topics (k)**\n",
    "\n",
    "**Number of topics (k)** defines the LDA model. Researchers must tell the model how many (k) prominent goal inference topics to sort each ‘bag of words’ document into. Problematically, several different k-values might work. Thus, we use a metric called perplexity to help us to determine the optimal number of topics. The utility in perplexity comes from comparing perplexity values across models with differing k-values to pinpoint the best model (i.e., the model with the lowest perplexity score). \n",
    "\n",
    "We recommend testing the perplexity of the model with a variety of k values, and then running the final model using the k-value with the selected perplexity score. **Model perplexity** is a frequently used metric that gauges how well a model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set model Hyper Parameters\n",
    "k = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "random_state=42\n",
    "update_every=1\n",
    "chunksize=1800\n",
    "passes=300\n",
    "iterations=850\n",
    "alpha='auto'\n",
    "eta= 'auto'\n",
    "per_word_topics=True\n",
    "\n",
    "lda_model_goals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Perplexity Scores of Training Dataset\n",
    "print(\"\\n***********************************************************************\")\n",
    "print(\"[INFO] goals Full Dataset LDA Results....\")\n",
    "print(\"***********************************************************************\")\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i in k:\n",
    "    lda_model_goals = LdaModel(corpus=corpus_goals,\n",
    "                                          id2word=dictionary_goals,\n",
    "                                          num_topics=i, \n",
    "                                          random_state=random_state,\n",
    "                                          update_every=update_every,\n",
    "                                          chunksize=chunksize,\n",
    "                                          passes=passes,\n",
    "                                          iterations=iterations,\n",
    "                                          alpha=alpha,\n",
    "                                          eta=eta,                                                            \n",
    "                                          per_word_topics=per_word_topics)\n",
    "\n",
    "    log_perplexity = lda_model_goals.log_perplexity(corpus_goals)\n",
    "    scores.append(log_perplexity)\n",
    "    print('\\nPerplexity (num_topics = {}): '.format(i), log_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Choosing Optimal Model (k) with Perplexity Scores\n",
    "\n",
    "#create Figure and Axes instances\n",
    "fig, ax = plt.subplots(1, figsize=(15,5))\n",
    "\n",
    "#plot\n",
    "topic_num = [n + 1 for n in range(len(scores))]\n",
    "ax.plot(topic_num, scores, color='b')\n",
    "\n",
    "#turn off y axis but keep labels \n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "plt.xticks(topic_num)\n",
    "\n",
    "#set title and grid\n",
    "plt.title('Model Perplexity Score by number of topics (k)', size=16)\n",
    "ax.set_xlabel('Number of topics (k)', size=12)\n",
    "plt.grid(True, axis='y', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, lower perplexity scores are indicative of increased model accuracy, and smaller k-values yield a more parsimonious set of topics. However, the perplexity score will often decrease as k increases. In these instances, it’s best to select the model that yields the lowest perplexity value before the values flatten out. \n",
    "\n",
    "**Note: when selecting the optimal number of topics, we need to find a balance between overfitting and underfitting the model**\n",
    "\n",
    "**Overfitting** (i.e., too many topics) can make it harder for human coders to label resulted topics since there is less coherence amongst the words in each topic. At the same time, resulting topics have less overlap in words.\n",
    "\n",
    "**Underfitting** (i.e., too few topics) doesn't produce enough variance, limiting options for statistical analyses. Labeling resulting topics becomes easier since topics have more coherent list of words comprising each topic. At the same time, resulting topics have higher overlap in used words that leads to increased variance in the distribution of topics in each document.\n",
    "\n",
    "**pyLDAvis visualization** of a model with selected value for k helps to asses Overfitting and Underfitting. Reading pyLDAvis:\n",
    "\n",
    "Left pane:\n",
    "- The area of each circle represents the prevalence of each topic over the entire corpus \n",
    "- The distance between the center of circles indicate the similarity between topics (i.e., inter-topic differences)\n",
    "\n",
    "Right pane:\n",
    "- If you hover over a particular topic on the left, the histogram on the right side lists the top 30 most relevant terms\n",
    "- The widths of the gray bars represent the corpus-wide frequencies of each term, and the widths of the red bars represent the topic-specific frequencies of each term\n",
    "- A slider at the top can adjust the relevance metric (λ); however, for our purposes, be sure it i set to λ = 1. For more information on the relevance metric, see library documentation. \n",
    "\n",
    "Documentation for this library can be found here: https://www.aclweb.org/anthology/W14-3110.pdf. \n",
    "\n",
    "**In the following steps we test LDA model hyper parameters for k in range [3-11].**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After looking at the perplexity scorews for k=1-12 topics, we can see that k=6 topics yielded the lowest perplexity value before the trend increased at k=7 topics. Accordingly, we proceeded with k=6 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing LDA Models and Parameters\n",
    "topic_number = 6\n",
    "random_state=42\n",
    "update_every=1\n",
    "chunksize=1800\n",
    "passes=300\n",
    "iterations=850\n",
    "alpha='auto'\n",
    "eta='auto'\n",
    "per_word_topics=True\n",
    "\n",
    "print(\"\\n***********************************************************************\")\n",
    "print(\"[INFO] goals Full Dataset LDA Results....\")\n",
    "print(\"***********************************************************************\")\n",
    "\n",
    "lda_model_goals = LdaModel(corpus=corpus_goals,\n",
    "                                      id2word=dictionary_goals,\n",
    "                                      num_topics=topic_number, \n",
    "                                      random_state=random_state,\n",
    "                                      update_every=update_every,\n",
    "                                      chunksize=chunksize,\n",
    "                                      passes=passes,\n",
    "                                      iterations=iterations,\n",
    "                                      alpha=alpha,\n",
    "                                      eta=eta,\n",
    "                                      per_word_topics=per_word_topics)\n",
    "\n",
    "print('\\nPerplexity (topic_number = {}): '.format(topic_number), lda_model_goals.log_perplexity(corpus_goals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyze Model results\n",
    "print(\"\\n***********************************************************************\")\n",
    "print(\"[INFO] goals Full Dataset Model Results....\")\n",
    "print(\"***********************************************************************\")\n",
    "\n",
    "print(\"\\n[INFO] Num_topics: {}\\n\".format(topic_number))\n",
    "topics = lda_model_goals.show_topics(num_topics=topic_number, num_words=10, log=True, formatted=True)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# print(\"goals.....k = 6...................\")\n",
    "# lda_display = pyLDAvis.gensim.prepare(lda_model_goals, corpus_goals, dictionary_goals)\n",
    "# pyLDAvis.display(lda_display)\n",
    "\n",
    "lda_display = pyLDAvis.gensim.prepare(lda_model_goals, corpus_goals, dictionary_goals)\n",
    "pyLDAvis.display(lda_display)\n",
    "\n",
    "lda_display = pyLDAvis.gensim.prepare(lda_model_goals, corpus_goals, dictionary_goals)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## However, the k=6 topic model was not deemed coherent by human coders (i.e., it did not explain the data well); thus we determined a k=9 topic LDA model would produce the next simplest model because k=9 topics yielded the lowest perplexity value before the trend flattened out at k=10 topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### k = 9 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing LDA Models and Parameters\n",
    "topic_number = 9\n",
    "random_state=42\n",
    "update_every=1\n",
    "chunksize=1800\n",
    "passes=300\n",
    "iterations=850\n",
    "alpha='auto'\n",
    "eta='auto'\n",
    "per_word_topics=True\n",
    "\n",
    "print(\"\\n***********************************************************************\")\n",
    "print(\"[INFO] goals Full Dataset LDA Results....\")\n",
    "print(\"***********************************************************************\")\n",
    "\n",
    "lda_model_goals = LdaModel(corpus=corpus_goals,\n",
    "                                      id2word=dictionary_goals,\n",
    "                                      num_topics=topic_number, \n",
    "                                      random_state=random_state,\n",
    "                                      update_every=update_every,\n",
    "                                      chunksize=chunksize,\n",
    "                                      passes=passes,\n",
    "                                      iterations=iterations,\n",
    "                                      alpha=alpha,\n",
    "                                      eta=eta,\n",
    "                                      per_word_topics=per_word_topics)\n",
    "\n",
    "print('\\nPerplexity (topic_number = {}): '.format(topic_number), lda_model_goals.log_perplexity(corpus_goals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyze Model results\n",
    "print(\"\\n***********************************************************************\")\n",
    "print(\"[INFO] goals Full Dataset Model Results....\")\n",
    "print(\"***********************************************************************\")\n",
    "\n",
    "print(\"\\n[INFO] Num_topics: {}\\n\".format(topic_number))\n",
    "topics = lda_model_goals.show_topics(num_topics=topic_number, num_words=10, log=True, formatted=True)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# print(\"goals.....k = 9...................\")\n",
    "# lda_display = pyLDAvis.gensim.prepare(lda_model_goals, corpus_goals, dictionary_goals)\n",
    "# pyLDAvis.display(lda_display)\n",
    "\n",
    "lda_display = pyLDAvis.gensim.prepare(lda_model_goals, corpus_goals, dictionary_goals)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Saving selected model results\n",
    "k=9 topics yielded the lowest perplexity value of the models that fit the data well before the trend flattened out at k=10 topics. We can also see that k=9 topics appear to be relatively spread out, with no overlapping topics. Thus, we determined a k=9-topic LDA model would produce the simplest model of the models that explain the data well\n",
    "\n",
    "\n",
    "To save results from the LDA model with selected parameters and number of topics we \n",
    "- rerun the model with k=9\n",
    "- generate a column that tells us which topic each response contributed the most to\n",
    "- save the analysis results to an excel file for topic validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing LDA Models and Parameters\n",
    "topic_number = 9\n",
    "random_state=42\n",
    "update_every=1\n",
    "\n",
    "chunksize=1800\n",
    "passes=300\n",
    "iterations=800\n",
    "alpha='auto'\n",
    "eta='auto'\n",
    "per_word_topics=True\n",
    "\n",
    "print(\"\\n***********************************************************************\")\n",
    "print(\"[INFO] goals Full Dataset LDA Results....\")\n",
    "print(\"***********************************************************************\")\n",
    "\n",
    "lda_model_goals = LdaModel(corpus=corpus_goals,\n",
    "                                      id2word=dictionary_goals,\n",
    "                                      num_topics=topic_number, \n",
    "                                      random_state=random_state,\n",
    "                                      update_every=update_every,\n",
    "                                      chunksize=chunksize,\n",
    "                                      passes=passes,\n",
    "                                      iterations=iterations,\n",
    "                                      alpha=alpha,\n",
    "                                      eta=eta,\n",
    "                                      per_word_topics=per_word_topics)\n",
    "\n",
    "print('\\nPerplexity (topic_number = {}): '.format(topic_number), lda_model_goals.log_perplexity(corpus_goals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## goals Model Results\n",
    "print(\"\\n***********************************************************************\")\n",
    "print(\"[INFO] Goal Inferences Model Results....\")\n",
    "print(\"***********************************************************************\")\n",
    "\n",
    "print(\"\\n[INFO] Number of topics: {}\\n\".format(topic_number))\n",
    "topics = lda_model_goals.show_topics(num_topics=topic_number, num_words=11, log=True, formatted=True)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# print(\"Goal Inferences .....k = 9...................\")\n",
    "# lda_display = pyLDAvis.gensim.prepare(lda_model_goals, corpus_goals, dictionary_goals)\n",
    "# pyLDAvis.display(lda_display)\n",
    "\n",
    "lda_display = pyLDAvis.gensim.prepare(lda_model_goals, corpus_goals, dictionary_goals)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a column that tells us which topic each response contributed the most to\n",
    "\n",
    "#define helpful function\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get the main topic of each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        \n",
    "        # Get the Dominant topic, Perc Contribution, and Keywords for each document\n",
    "        raw_frame = {}\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j==0:\n",
    "                raw_frame['Dominant'] = topic_num\n",
    "\n",
    "            raw_frame['Topic' + str(topic_num)] = round(prop_topic, 8) # the '8' here should be one number smaller than your k # (e.g., 9-1=8)\n",
    "            \n",
    "        df = pd.DataFrame(data=raw_frame, index=[0])\n",
    "        sent_topics_df = sent_topics_df.append(df)\n",
    "\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords_goals = format_topics_sentences(ldamodel=lda_model_goals, \n",
    "                                                                   corpus=corpus_goals, \n",
    "                                                                   texts=goals_stopwords)\n",
    "\n",
    "#rename index of the dataframe\n",
    "#df_dominant_topic_goals = df_topic_sents_keywords_goals.reset_index()\n",
    "df_dominant_topic_goals = df_topic_sents_keywords_goals.reset_index(drop=True)\n",
    "\n",
    "df_dominant_topic_goals.index.name='Document_No';\n",
    "\n",
    "print(df_dominant_topic_goals.head(117))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic_goals.head(812).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a data frame to export the results into\n",
    "\n",
    "lda_topics_goals = np.array(df_dominant_topic_goals['Dominant'])\n",
    "topic0_contrib_lda_topics_goals = np.array(df_dominant_topic_goals['Topic0'])\n",
    "topic1_contrib_lda_topics_goals = np.array(df_dominant_topic_goals['Topic1'])\n",
    "topic2_contrib_lda_topics_goals = np.array(df_dominant_topic_goals['Topic2'])\n",
    "topic3_contrib_lda_topics_goals = np.array(df_dominant_topic_goals['Topic3'])\n",
    "topic4_contrib_lda_topics_goals = np.array(df_dominant_topic_goals['Topic4'])\n",
    "topic5_contrib_lda_topics_goals = np.array(df_dominant_topic_goals['Topic5'])\n",
    "topic6_contrib_lda_topics_goals = np.array(df_dominant_topic_goals['Topic6'])\n",
    "topic7_contrib_lda_topics_goals = np.array(df_dominant_topic_goals['Topic7'])\n",
    "topic8_contrib_lda_topics_goals = np.array(df_dominant_topic_goals['Topic8'])\n",
    "\n",
    "goals = np.array(data['goals'])\n",
    "\n",
    "results = { \n",
    "    'goals': goals, \n",
    "    'lda_topics_goals': lda_topics_goals, \n",
    "    'topic0_contrib_lda_topics_goals':topic0_contrib_lda_topics_goals,\n",
    "    'topic1_contrib_lda_topics_goals':topic1_contrib_lda_topics_goals,\n",
    "    'topic2_contrib_lda_topics_goals':topic2_contrib_lda_topics_goals,\n",
    "    'topic3_contrib_lda_topics_goals':topic3_contrib_lda_topics_goals,\n",
    "    'topic4_contrib_lda_topics_goals':topic4_contrib_lda_topics_goals,\n",
    "    'topic5_contrib_lda_topics_goals':topic5_contrib_lda_topics_goals,\n",
    "    'topic6_contrib_lda_topics_goals':topic6_contrib_lda_topics_goals,\n",
    "    'topic7_contrib_lda_topics_goals':topic7_contrib_lda_topics_goals,\n",
    "    'topic8_contrib_lda_topics_goals':topic8_contrib_lda_topics_goals\n",
    "}\n",
    "\n",
    "frame = pd.DataFrame(results, columns = [\n",
    "                                        'goals', 'lda_topics_goals', \n",
    "                                        'topic0_contrib_lda_topics_goals',\n",
    "                                        'topic1_contrib_lda_topics_goals',\n",
    "                                        'topic2_contrib_lda_topics_goals',\n",
    "                                        'topic3_contrib_lda_topics_goals',\n",
    "                                        'topic4_contrib_lda_topics_goals',\n",
    "                                        'topic5_contrib_lda_topics_goals',\n",
    "                                        'topic6_contrib_lda_topics_goals',\n",
    "                                        'topic7_contrib_lda_topics_goals',\n",
    "                                        'topic8_contrib_lda_topics_goals'\n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export results to an .xlsx file\n",
    "frame.to_excel(\"./output/LDA_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.to_csv(\"./output/LDA_results.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
